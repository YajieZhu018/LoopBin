{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 22:01:18.077478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs\n",
      "2024-03-12 22:01:18.077518: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-12 22:01:35.645889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs\n",
      "2024-03-12 22:01:35.645931: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-03-12 22:01:35.645953: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sgiz001): /proc/driver/nvidia/version does not exist\n",
      "2024-03-12 22:01:35.646198: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 10  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 22:01:38.059064: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-03-12 22:01:38.059607: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2100000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 15s 5ms/step - loss: 0.4399 - val_loss: 0.2457\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2308 - val_loss: 0.1976\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1934 - val_loss: 0.1801\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1792 - val_loss: 0.1725\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1729 - val_loss: 0.1687\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1691 - val_loss: 0.1664\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1672 - val_loss: 0.1646\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.1631\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1644 - val_loss: 0.1618\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1630 - val_loss: 0.1607\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1619 - val_loss: 0.1597\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1609 - val_loss: 0.1587\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1599 - val_loss: 0.1579\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1595 - val_loss: 0.1573\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1588 - val_loss: 0.1566\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1581 - val_loss: 0.1562\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1577 - val_loss: 0.1558\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1576 - val_loss: 0.1554\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1567 - val_loss: 0.1551\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1565 - val_loss: 0.1550\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1563 - val_loss: 0.1547\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1561 - val_loss: 0.1545\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1558 - val_loss: 0.1544\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1555 - val_loss: 0.1541\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1555 - val_loss: 0.1539\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1555 - val_loss: 0.1539\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1558 - val_loss: 0.1537\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1552 - val_loss: 0.1536\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1551 - val_loss: 0.1536\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1554 - val_loss: 0.1535\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1549 - val_loss: 0.1534\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1551 - val_loss: 0.1534\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1545 - val_loss: 0.1532\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1546 - val_loss: 0.1532\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1548 - val_loss: 0.1530\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1543 - val_loss: 0.1530\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1544 - val_loss: 0.1529\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1543 - val_loss: 0.1528\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1543 - val_loss: 0.1528\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1541 - val_loss: 0.1527\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1542 - val_loss: 0.1526\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1543 - val_loss: 0.1526\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1539 - val_loss: 0.1525\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1540 - val_loss: 0.1525\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1536 - val_loss: 0.1524\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1534 - val_loss: 0.1524\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1534 - val_loss: 0.1524\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1535 - val_loss: 0.1523\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1537 - val_loss: 0.1523\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1534 - val_loss: 0.1522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b3f58d24890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cluster_model\n",
    "train_dataset = encoder.predict(x_train)\n",
    "val_dataset = encoder.predict(x_test)\n",
    "# prepare training sets\n",
    "batch_size = 256\n",
    "# Prepare the training dataset.\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val_dataset)\n",
    "val_data= val_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "mcluster = KMeans(n_clusters=10).fit(encoder.predict(x_train))\n",
    "kmeans_center = mcluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'rbf/centers:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[ 7.975895 ,  9.910124 , 13.327782 ,  5.143341 ,  5.0230947,\n",
      "         5.5389605,  7.3247957,  5.4877477,  5.892302 ,  9.291711 ],\n",
      "       [ 6.661634 , 17.214285 , 10.850711 ,  2.715038 ,  8.289419 ,\n",
      "         5.0946665,  7.945873 , 12.245331 ,  9.6540575, 16.247677 ],\n",
      "       [ 7.338706 , 10.486046 , 17.165771 ,  8.093777 ,  6.6751246,\n",
      "        17.66114  ,  9.146444 ,  6.7045097,  9.15157  ,  5.891575 ],\n",
      "       [ 7.273858 ,  6.4048142, 16.246181 ,  2.194916 ,  7.656436 ,\n",
      "        13.538785 , 12.8661375, 13.701747 ,  6.318465 , 18.625742 ],\n",
      "       [ 8.477515 ,  9.393585 ,  5.2010345,  5.5290318,  6.36283  ,\n",
      "         6.074559 ,  9.928228 ,  3.847815 ,  4.3231373,  4.5977755],\n",
      "       [ 9.480784 , 13.551057 , 20.534431 ,  7.7427254,  7.102842 ,\n",
      "         8.995188 , 14.303588 ,  9.763858 ,  7.190873 , 13.270897 ],\n",
      "       [ 8.251628 , 18.827759 , 20.95652  ,  5.4256516,  9.853117 ,\n",
      "        14.646816 , 18.415344 , 18.166853 ,  9.927198 , 17.25164  ],\n",
      "       [ 3.2129064, 10.483582 ,  5.626787 ,  4.007539 ,  4.4201813,\n",
      "         8.361394 ,  6.2432036, 10.292126 ,  6.73998  ,  5.6128874],\n",
      "       [ 4.4884424, 14.251455 , 11.804673 ,  4.172546 ,  8.057917 ,\n",
      "         8.122291 , 16.171764 , 10.73932  ,  7.175953 ,  9.133113 ],\n",
      "       [ 4.782527 , 12.100718 ,  9.72393  ,  9.421803 ,  5.1999598,\n",
      "        13.835413 ,  9.287653 , 16.66422  ,  8.876548 ,  5.4005423]],\n",
      "      dtype=float32)>, <tf.Variable 'rbf/log_sigmas:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n",
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n",
      "Training loss (for one batch) at step 120: 4.3219\n",
      "Training loss (for one batch) at step 130: 4.3219\n",
      "Training loss (for one batch) at step 140: 4.3219\n",
      "Training loss (for one batch) at step 150: 4.3219\n",
      "Training loss (for one batch) at step 160: 4.3219\n",
      "Training loss (for one batch) at step 170: 4.3219\n",
      "Training loss (for one batch) at step 180: 4.3219\n",
      "Training loss (for one batch) at step 190: 4.3219\n",
      "Training loss (for one batch) at step 200: 4.3219\n",
      "Training loss (for one batch) at step 210: 4.3219\n",
      "Training loss (for one batch) at step 220: 4.3219\n",
      "Training loss (for one batch) at step 230: 4.3219\n",
      "Epoch 1, Loss: 4.3219146728515625\n",
      "Validation Loss: 4.321929454803467\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n",
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n",
      "Training loss (for one batch) at step 120: 4.3219\n",
      "Training loss (for one batch) at step 130: 4.3219\n",
      "Training loss (for one batch) at step 140: 4.3219\n",
      "Training loss (for one batch) at step 150: 4.3219\n",
      "Training loss (for one batch) at step 160: 4.3219\n",
      "Training loss (for one batch) at step 170: 4.3219\n",
      "Training loss (for one batch) at step 180: 4.3219\n",
      "Training loss (for one batch) at step 190: 4.3219\n",
      "Training loss (for one batch) at step 200: 4.3219\n",
      "Training loss (for one batch) at step 210: 4.3219\n",
      "Training loss (for one batch) at step 220: 4.3219\n",
      "Training loss (for one batch) at step 230: 4.3219\n",
      "Epoch 2, Loss: 4.3219146728515625\n",
      "Validation Loss: 4.321929454803467\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n",
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n",
      "Training loss (for one batch) at step 120: 4.3219\n",
      "Training loss (for one batch) at step 130: 4.3219\n",
      "Training loss (for one batch) at step 140: 4.3219\n",
      "Training loss (for one batch) at step 150: 4.3219\n",
      "Training loss (for one batch) at step 160: 4.3219\n",
      "Training loss (for one batch) at step 170: 4.3219\n",
      "Training loss (for one batch) at step 180: 4.3219\n",
      "Training loss (for one batch) at step 190: 4.3219\n",
      "Training loss (for one batch) at step 200: 4.3219\n",
      "Training loss (for one batch) at step 210: 4.3219\n",
      "Training loss (for one batch) at step 220: 4.3219\n",
      "Training loss (for one batch) at step 230: 4.3219\n",
      "Epoch 3, Loss: 4.3219146728515625\n",
      "Validation Loss: 4.321929454803467\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n",
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n",
      "Training loss (for one batch) at step 120: 4.3219\n",
      "Training loss (for one batch) at step 130: 4.3219\n",
      "Training loss (for one batch) at step 140: 4.3219\n",
      "Training loss (for one batch) at step 150: 4.3219\n",
      "Training loss (for one batch) at step 160: 4.3219\n",
      "Training loss (for one batch) at step 170: 4.3219\n",
      "Training loss (for one batch) at step 180: 4.3219\n",
      "Training loss (for one batch) at step 190: 4.3219\n",
      "Training loss (for one batch) at step 200: 4.3219\n",
      "Training loss (for one batch) at step 210: 4.3219\n",
      "Training loss (for one batch) at step 220: 4.3219\n",
      "Training loss (for one batch) at step 230: 4.3219\n",
      "Epoch 4, Loss: 4.3219146728515625\n",
      "Validation Loss: 4.321929454803467\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n",
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n",
      "Training loss (for one batch) at step 120: 4.3219\n",
      "Training loss (for one batch) at step 130: 4.3219\n",
      "Training loss (for one batch) at step 140: 4.3219\n",
      "Training loss (for one batch) at step 150: 4.3219\n",
      "Training loss (for one batch) at step 160: 4.3219\n",
      "Training loss (for one batch) at step 170: 4.3219\n",
      "Training loss (for one batch) at step 180: 4.3219\n",
      "Training loss (for one batch) at step 190: 4.3219\n",
      "Training loss (for one batch) at step 200: 4.3219\n",
      "Training loss (for one batch) at step 210: 4.3219\n",
      "Training loss (for one batch) at step 220: 4.3219\n",
      "Training loss (for one batch) at step 230: 4.3219\n",
      "Epoch 5, Loss: 4.3219146728515625\n",
      "Validation Loss: 4.321929454803467\n",
      "Training loss (for one batch) at step 0: 4.3219\n",
      "Training loss (for one batch) at step 10: 4.3219\n",
      "Training loss (for one batch) at step 20: 4.3219\n",
      "Training loss (for one batch) at step 30: 4.3219\n",
      "Training loss (for one batch) at step 40: 4.3219\n",
      "Training loss (for one batch) at step 50: 4.3219\n",
      "Training loss (for one batch) at step 60: 4.3219\n",
      "Training loss (for one batch) at step 70: 4.3219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 80: 4.3219\n",
      "Training loss (for one batch) at step 90: 4.3219\n",
      "Training loss (for one batch) at step 100: 4.3219\n",
      "Training loss (for one batch) at step 110: 4.3219\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/local/jobs/19611351/ipykernel_41450/3530151229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClusterModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/LoopHigh/src/model/cluster_model.py\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# loop through batches of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LoopHigh/src/model/cluster_model.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mclustering_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoft_sil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoft_clustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_TileGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_TileGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m   \u001b[0;34m\"\"\"Sum reduces grad along the tiled dimensions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m   \u001b[0;31m# We interleave multiples and input_shape to get split_shape,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m   \u001b[0;31m# reshape grad to split_shape, and reduce along all even\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \"\"\"\n\u001b[0;32m--> 650\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/loophigh/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9173\u001b[0m   \u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9174\u001b[0m   \u001b[0mtld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9175\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9177\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import objectives\n",
    "import importlib\n",
    "importlib.reload(cluster_model)\n",
    "latent_dim = train_dataset.shape[1]\n",
    "n_cluster = 10\n",
    "n_epochs = 10\n",
    "model = cluster_model.ClusterModel(latent_dim, n_cluster, n_epochs, kmeans_center)\n",
    "#print(model.trainable_variables)\n",
    "model.train_process([train_data,val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'rbf/centers:0' shape=(10, 32) dtype=float32, numpy=\n",
      "array([[11.3268385, 13.217806 , 15.067872 ,  9.923792 , 10.050951 ,\n",
      "        15.483904 ,  9.005194 , 12.865871 ,  9.651903 , 15.388498 ,\n",
      "        13.171017 , 10.945674 ,  9.88551  ,  7.4695635,  8.45224  ,\n",
      "        10.4318075,  9.594649 , 13.882483 ,  8.78408  , 22.044886 ,\n",
      "         9.240414 , 12.329746 ,  9.1415   , 13.056038 ,  8.341825 ,\n",
      "         8.286819 , 10.814162 ,  8.489073 , 10.325443 , 11.7908745,\n",
      "         9.763215 , 11.290315 ],\n",
      "       [ 8.318989 ,  9.310397 , 10.606131 ,  7.0333414,  7.140271 ,\n",
      "        10.677285 ,  7.6182346,  8.539663 ,  7.32878  , 10.935629 ,\n",
      "         8.789523 ,  8.046115 ,  7.2707663,  6.0805016,  6.909371 ,\n",
      "         7.2568893,  8.053214 ,  9.189574 ,  6.7381964, 16.071407 ,\n",
      "         5.894759 ,  8.535564 ,  6.592586 ,  9.7597275,  7.2308526,\n",
      "         6.4987273,  8.228117 ,  6.3719645,  7.125444 ,  9.003385 ,\n",
      "         6.8816667,  9.126386 ],\n",
      "       [ 6.706127 ,  7.8942776,  9.157573 ,  7.047304 ,  7.1160927,\n",
      "         5.9179487, 13.908477 ,  7.3831444,  6.3662815,  8.04196  ,\n",
      "         6.4263654,  6.4909735,  8.0775175,  3.6033099,  7.0767117,\n",
      "         8.470832 , 10.976973 ,  7.598481 ,  5.796476 ,  6.7373285,\n",
      "         8.143913 ,  6.811737 ,  6.512741 ,  6.538457 ,  5.7362547,\n",
      "         7.3087535,  8.918907 ,  9.928394 ,  4.8226953,  6.3565674,\n",
      "         6.363106 ,  6.6337867],\n",
      "       [10.71001  , 11.496228 ,  9.561535 ,  9.132234 , 10.474194 ,\n",
      "         9.024569 , 20.201561 , 11.203446 ,  9.4533415, 10.424116 ,\n",
      "         9.216714 ,  9.437728 , 10.113897 ,  4.896119 ,  8.597006 ,\n",
      "        11.594629 , 11.762026 , 12.388392 ,  6.9916406, 11.189592 ,\n",
      "        10.713578 ,  7.8381834, 11.281938 ,  9.525726 ,  6.9366183,\n",
      "         8.772645 , 10.889184 , 13.767666 ,  8.506936 ,  9.494286 ,\n",
      "         7.9296627,  9.834517 ],\n",
      "       [10.640568 , 12.21385  ,  9.769995 , 13.897703 ,  9.871713 ,\n",
      "        12.257746 , 10.571028 , 10.9884205, 11.635444 , 11.376936 ,\n",
      "        11.904464 , 12.872729 , 11.270125 ,  7.3815117,  8.910726 ,\n",
      "        11.545518 ,  7.4347663, 18.62498  , 10.721688 ,  8.786556 ,\n",
      "        10.538079 ,  9.320368 , 11.327008 , 11.956635 ,  9.936041 ,\n",
      "        11.499258 , 12.5932865, 11.889117 , 10.079954 , 10.319544 ,\n",
      "         9.028923 , 11.042367 ],\n",
      "       [ 9.698107 , 16.36084  ,  9.659273 , 14.187988 ,  9.550414 ,\n",
      "        11.617878 , 11.442201 , 11.7614975,  5.077924 , 16.681204 ,\n",
      "        12.41805  ,  7.4029026,  7.920101 ,  4.5865088,  7.121039 ,\n",
      "         8.270831 ,  5.186111 , 18.637257 ,  8.610791 , 16.83018  ,\n",
      "         7.5852585,  8.856907 , 17.896484 ,  9.846753 , 10.18475  ,\n",
      "         7.929961 , 12.439108 ,  8.817202 ,  7.1446433,  9.933312 ,\n",
      "        10.438101 , 11.291744 ],\n",
      "       [ 7.483096 ,  5.4327874,  5.205105 ,  6.346805 ,  4.746564 ,\n",
      "         5.35935  ,  6.0219965,  5.409235 ,  5.9882755,  5.8857317,\n",
      "         4.9545093,  5.4186926,  6.269373 ,  7.0491557,  4.473529 ,\n",
      "         6.6584587,  6.664546 ,  7.2694244,  5.880416 ,  6.1836877,\n",
      "         5.392872 ,  4.622077 ,  7.026451 ,  5.709917 ,  4.3394394,\n",
      "         6.285868 ,  4.570314 ,  4.906317 ,  6.5030203,  6.7670755,\n",
      "         5.3787427,  4.8271565],\n",
      "       [ 8.779486 ,  9.586046 ,  6.9533772, 13.142272 ,  7.0424614,\n",
      "         9.11306  ,  7.5853786,  7.6673145,  8.948083 ,  8.610039 ,\n",
      "         8.932435 ,  9.114175 ,  7.457963 ,  5.5326476,  6.665167 ,\n",
      "         7.6210966,  7.0139856, 14.119746 ,  8.678677 ,  6.6214085,\n",
      "         6.833112 ,  7.1418915,  9.26122  ,  8.726721 ,  8.427636 ,\n",
      "         7.935612 , 10.502224 ,  7.3365526,  6.647102 ,  7.5767784,\n",
      "         6.6565   ,  8.198888 ],\n",
      "       [14.187557 , 17.737139 , 13.469137 , 16.949108 , 13.015269 ,\n",
      "        17.134846 , 15.073376 , 15.619731 , 11.878923 , 18.117699 ,\n",
      "        16.7543   , 14.593054 , 13.609231 ,  7.8467093, 10.5539055,\n",
      "        13.9598875,  9.366118 , 23.9741   , 11.2726345, 17.387825 ,\n",
      "        13.323021 , 12.169371 , 15.772689 , 16.761295 , 12.621789 ,\n",
      "        11.438692 , 15.855192 , 13.97757  , 12.54229  , 13.492122 ,\n",
      "        11.764827 , 15.018704 ],\n",
      "       [ 8.828142 ,  7.5695615,  6.68591  ,  6.939167 ,  6.8884964,\n",
      "         9.2944   , 11.089916 ,  9.521759 ,  9.022445 ,  7.7790337,\n",
      "         7.3898783,  8.057974 ,  7.7250524,  7.2301865,  7.35122  ,\n",
      "         9.366269 ,  5.805482 , 10.407293 ,  7.3342743,  8.777656 ,\n",
      "         8.811952 ,  6.899295 , 11.613445 ,  8.228049 ,  4.955453 ,\n",
      "         9.382574 ,  6.1249595,  9.208779 ,  9.230313 ,  8.6464615,\n",
      "         6.5325108,  8.317166 ]], dtype=float32)>, <tf.Variable 'rbf/log_sigmas:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loophigh",
   "language": "python",
   "name": "loophigh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
